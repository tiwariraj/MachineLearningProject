runApp('~/R/Code/IRR Data Science Build/Shiny Apps/flights_final')
runApp('~/R/Code/IRR Data Science Build/Shiny Apps/flights_final')
runApp('~/R/Code/IRR Data Science Build/Shiny Apps/flights_final')
runApp('~/R/Code/IRR Data Science Build/Shiny Apps/flights_final')
goods=goods %>%
filter(MarketName=="Atlanta, GA")
goods
runApp('~/R/Code/IRR Data Science Build/Shiny Apps/flights_final')
runApp('~/R/Code/IRR Data Science Build/Shiny Apps/flights_final')
goods
unique(x$Classification)
goods <- fread(file = "goods.csv")
runApp('~/R/Code/IRR Data Science Build/Shiny Apps/flights_final')
runApp('~/R/Code/IRR Data Science Build/Shiny Apps/flights_final')
runApp('~/R/Code/IRR Data Science Build/Shiny Apps/flights_final')
runApp('~/R/Code/IRR Data Science Build/Shiny Apps/flights_final')
runApp('~/R/Code/IRR Data Science Build/Shiny Apps/flights_final')
goods
goods %>% filter(MarketName,Classification,MonthlyCost) %>%
summarise(
AvgCityTypeCost = mean(MonthlyCost,na.rm =TRUE),
Index = (MonthlyCost/mean(MonthlyCost,na.rm =TRUE)*100))
goods %>%
summarise(
AvgCityTypeCost = mean(MonthlyCost,na.rm =TRUE),
Index = (MonthlyCost/mean(MonthlyCost,na.rm =TRUE)*100))
runApp('~/R/Code/IRR Data Science Build/Shiny Apps/flights_final')
runApp('~/R/Code/IRR Data Science Build/Shiny Apps/flights_final')
runApp('~/R/Code/IRR Data Science Build/Shiny Apps/flights_final')
runApp('~/R/Code/IRR Data Science Build/Shiny Apps/flights_final')
runApp('~/R/Code/IRR Data Science Build/Shiny Apps/flights_final')
runApp('~/R/Code/IRR Data Science Build/Shiny Apps/flights_final')
runApp('~/R/Code/IRR Data Science Build/Shiny Apps/flights_final')
runApp('~/R/Code/IRR Data Science Build/Shiny Apps/flights_final')
runApp('~/R/Code/IRR Data Science Build/Shiny Apps/flights_final')
runApp('~/R/Code/IRR Data Science Build/Shiny Apps/flights_final')
runApp('~/R/Code/IRR Data Science Build/Shiny Apps/flights_final')
shiny::runApp('~/R/Code/IRR Data Science Build/Shiny Apps/flights_final')
shiny::runApp('~/R/Code/IRR Data Science Build/Shiny Apps/CostofLivingProject')
library(gbm)
#Fitting 10,0
set.seed(0)
boost.boston = gbm(medv ~ ., data = Boston[train, ],
distribution = "gaussian",
n.trees = 10000,
interaction.depth = 4)
library(MASS)
help(Boston)
set.seed(0)
boost.boston = gbm(medv ~ ., data = Boston[train, ],
distribution = "gaussian",
n.trees = 10000,
interaction.depth = 4)
library(MASS)
help(Boston)
#Creating a training set on 70% of the data.
set.seed(0)
train = sample(1:nrow(Boston), 7*nrow(Boston)/10)
#Training the tree to predict the median value of owner-occupied homes (in $1k).
tree.boston = tree(medv ~ ., Boston, subset = train)
summary(tree.boston)
#Visually inspecting the regression tree.
plot(tree.boston)
text(tree.boston, pretty = 0)
#Performing cross-validation.
set.seed(0)
cv.boston = cv.tree(tree.boston)
par(mfrow = c(1, 2))
plot(cv.boston$size, cv.boston$dev, type = "b",
xlab = "Terminal Nodes", ylab = "RSS")
plot(cv.boston$k, cv.boston$dev, type  = "b",
xlab = "Alpha", ylab = "RSS")
#Pruning the tree to have 4 terminal nodes.
prune.boston = prune.tree(tree.boston, best = 4)
par(mfrow = c(1, 1))
plot(prune.boston)
text(prune.boston, pretty = 0)
#Calculating and assessing the MSE of the test data on the overall tree.
yhat = predict(tree.boston, newdata = Boston[-train, ])
yhat
boston.test = Boston[-train, "medv"]
boston.test
plot(yhat, boston.test)
abline(0, 1)
mean((yhat - boston.test)^2)
#Calculating and assessing the MSE of the test data on the pruned tree.
yhat = predict(prune.boston, newdata = Boston[-train, ])
yhat
plot(yhat, boston.test)
abline(0, 1)
mean((yhat - boston.test)^2)
##################################
#####Bagging & Random Forests#####
##################################
library(randomForest)
#Fitting an initial random forest to the training subset.
set.seed(0)
rf.boston = randomForest(medv ~ ., data = Boston, subset = train, importance = TRUE)
rf.boston
#The MSE and percent variance explained are based on out-of-bag estimates,
#yielding unbiased error estimates. The model reports that mtry = 4, which is
#the number of variables randomly chosen at each split. Since we have 13 overall
#variables, we could try all 13 possible values of mtry. We will do so, record
#the results, and make a plot.
#Varying the number of variables used at each step of the random forest procedure.
set.seed(0)
oob.err = numeric(13)
for (mtry in 1:13) {
fit = randomForest(medv ~ ., data = Boston[train, ], mtry = mtry)
oob.err[mtry] = fit$mse[500]
cat("We're performing iteration", mtry, "\n")
}
#Visualizing the OOB error.
plot(1:13, oob.err, pch = 16, type = "b",
xlab = "Variables Considered at Each Split",
ylab = "OOB Mean Squared Error",
main = "Random Forest OOB Error Rates\nby # of Variables")
#Can visualize a variable importance plot.
importance(rf.boston)
varImpPlot(rf.boston)
library(gbm)
#Fitting 10,000 trees with a depth of 4.
set.seed(0)
boost.boston = gbm(medv ~ ., data = Boston[train, ],
distribution = "gaussian",
n.trees = 10000,
interaction.depth = 4)
#Inspecting the relative influence.
par(mfrow = c(1, 1))
summary(boost.boston)
#Let√¢s make a prediction on the test set. With boosting, the number of trees is
#a tuning parameter; having too many can cause overfitting. In general, we should
#use cross validation to select the number of trees. Instead, we will compute the
#test error as a function of the number of trees and make a plot for illustrative
#purposes.
n.trees = seq(from = 100, to = 10000, by = 100)
predmat = predict(boost.boston, newdata = Boston[-train, ], n.trees = n.trees)
dim(predmat)
predmat
par(mfrow = c(1, 1))
berr = with(Boston[-train, ], apply((predmat - medv)^2, 2, mean))
plot(n.trees, berr, pch = 16,
ylab = "Mean Squared Error",
xlab = "# Trees",
main = "Boosting Test Error")
#Include the best OOB error from the random forest.
abline(h = min(oob.err), col = "red")
set.seed(0)
boost.boston2 = gbm(medv ~ ., data = Boston[train, ],
distribution = "gaussian",
n.trees = 10000,
interaction.depth = 4,
shrinkage = 0.1)
predmat2 = predict(boost.boston2, newdata = Boston[-train, ], n.trees = n.trees)
berr2 = with(Boston[-train, ], apply((predmat2 - medv)^2, 2, mean))
plot(n.trees, berr2, pch = 16,
ylab = "Mean Squared Error",
xlab = "# Trees",
main = "Boosting Test Error")
install.packages("tabplot")
install.packages("lsr")
knitr::opts_chunk$set(echo = TRUE,
cache = TRUE,
warning = FALSE,
message = FALSE,
tidy=FALSE,
fig.height=6,
fig.width=10)
# setwd("~/Workspace/kaggle/housePriceAmes/")
setwd("~/NYCDSA/Bootcamp/Projects/Machine Learning/Data/DataScienceCaseStudy")
## load training data
house_train <- read.csv("./train.csv",
header = TRUE,
na.strings = "",
stringsAsFactors = FALSE)
## load test data
house_test <- read.csv("./test.csv",
header = TRUE,
na.strings = "",
stringsAsFactors = FALSE)
dim(house_train)
dim(house_test)
str(house_train)
## get levels of categorical features from data description
factorLevel <- list()
conn <- file("./data_description.txt", open="r")
f <-readLines(conn)
for (line in f){
if(!grepl("^[[:blank:]]", line) & grepl(": ", line)) {
col_name <<- trimws(gsub(":.*", "", line))
} else {
level <- trimws(gsub("\t.*", "", line))
if (level != "") {
factorLevel[[col_name]] <- c(factorLevel[[col_name]], level)
}
}
}
close(conn)
print(factorLevel[1:6])
## check if levels in description cover unique data values
for (varname in names(factorLevel)) {
levelDiff <- setdiff(unique(house_train[[varname]]),
factorLevel[[varname]])
if(length(levelDiff)) {
print(paste(varname,
paste(levelDiff, collapse = ", "),
sep = ": "))
}
}
## fix those levels that don't match with data
## ignore "NA" as they will be considered as missing when converting categorical to factors
unique(house_train$MSZoning)
factorLevel$MSZoning
factorLevel$MSZoning[2] <- "C (all)"
unique(house_train$Neighborhood)
factorLevel$Neighborhood
factorLevel$Neighborhood[13] <- "NAmes"
unique(house_train$BldgType)
factorLevel$BldgType
factorLevel$BldgType[c(2,3,5)] <- c("2fmCon","Duplex","Twnhs")
unique(house_train$Exterior2nd)
factorLevel$Exterior2nd
factorLevel$Exterior2nd[c(17,6,3)] <- c("Wd Shng","CmentBd","Brk Cmn")
## Get levels that only appear in the dataset
for (varname in names(factorLevel)) {
factorLevel[[varname]] <- intersect(factorLevel[[varname]],
unique(house_train[[varname]]))
}
## Re-run the previous cell to double check
## convert column datatype to numeric / factor
## On training dataset
for (varname in names(house_train)[-1]) {
if (varname %in% names(factorLevel)) {
house_train[[varname]] <- factor(house_train[[varname]],
levels = factorLevel[[varname]])
} else {
house_train[[varname]] <- as.numeric(house_train[[varname]])
}
}
## On testing dataset
for (varname in names(house_test)[-1]) {
if (varname %in% names(factorLevel)) {
house_test[[varname]] <- factor(house_test[[varname]],
levels = factorLevel[[varname]])
} else {
house_test[[varname]] <- as.numeric(house_test[[varname]])
}
}
house_train$Id <- NULL
rownames(house_test) <- house_test$Id
house_test$Id <- NULL
save(house_train, house_test, file = "./house_loaded.RData")
library(ggplot2)
library(gridExtra)
library(tabplot)
library(lsr)
library(corrplot)
library(dplyr)
rm(list = ls())
load("./house_loaded.RData")
## histogram on SalePrice
grid.arrange(ggplot(house_train) +
geom_histogram(aes(SalePrice), bins = 20),
ggplot(house_train) +
geom_histogram(aes(log(SalePrice + 1)), bins = 20),
ncol = 2)
## table plot all features on sortded SalePrice
colMtx <- matrix(names(house_train)[1:length(house_train)-1], nrow = 8)
for (i in 1:ncol(colMtx)) {
tableplot(house_train,
select_string = c(colMtx[,i], "SalePrice"),
sortCol = "SalePrice", decreasing = TRUE,
nBins = 30)
}
library(VIM)
library(caret)
library(RANN)
## check missing values
col_missing <- names(house_train)[colSums(is.na(house_train)) > 0]
aggr(house_train[,col_missing], prop = F, numbers = T)
Filter(function(x) x > 0, colSums(is.na(house_train)))
col_missing <- names(house_test)[colSums(is.na(house_test)) > 0]
aggr(house_test[,col_missing], prop = F, numbers = T)
Filter(function(x) x > 0, colSums(is.na(house_test)))
## table plot all features on sortded SalePrice
tableplot(house_train %>% select(starts_with("Garage")),
decreasing = TRUE,
nBins = 18,
colorNA = "#FF1414", colorNA_num = "#FF1414")
tableplot(house_test %>% select(starts_with("Garage")),
decreasing = TRUE,
nBins = 19,
colorNA = "#FF1414", colorNA_num = "#FF1414")
grid.arrange(
ggplot(house_train,
aes(YearBuilt, ifelse(is.na(GarageYrBlt), YearBuilt, GarageYrBlt))) +
geom_point(aes(color = is.na(GarageYrBlt))),
ggplot(house_test,
aes(YearBuilt, ifelse(is.na(GarageYrBlt), YearBuilt, GarageYrBlt))) +
geom_point(aes(color = is.na(GarageYrBlt))),
ncol = 2
)
## Fix outlier
house_test$GarageYrBlt[which(house_test$GarageYrBlt == 2207)] <- 2007
## Create new feature: hasGarage
## Impute GarageYrBlt with YearBuilt
house_train <- house_train %>%
mutate(hasGarage = ifelse(is.na(GarageYrBlt), 0, 1),
GarageBlt = ifelse(is.na(GarageYrBlt), 0, GarageYrBlt - YearBuilt)) %>%
select(-GarageYrBlt)
house_test <- house_test %>%
mutate(hasGarage = ifelse(is.na(GarageYrBlt), 0, 1),
GarageBlt = ifelse(is.na(GarageYrBlt), 0, GarageYrBlt - YearBuilt)) %>%
select(-GarageYrBlt)
grid.arrange(
ggplot(house_train, aes(YearBuilt, GarageBlt)) +
geom_point(aes(color = as.factor(hasGarage))),
ggplot(house_test, aes(YearBuilt, GarageBlt)) +
geom_point(aes(color = as.factor(hasGarage))),
ncol = 2
)
grid.arrange(
ggplot(house_train, aes(YearBuilt, YearRemodAdd)) +
geom_point(aes(color = (YearRemodAdd == YearBuilt))),
ggplot(house_test, aes(YearBuilt, YearRemodAdd)) +
geom_point(aes(color = (YearRemodAdd == YearBuilt))),
ncol = 2
)
house_train <- house_train %>%
mutate(isRemod = ifelse(YearRemodAdd == YearBuilt, 0, 1),
RemodAdd = YearRemodAdd - YearBuilt) %>%
select(-YearRemodAdd)
house_test <- house_test %>%
mutate(isRemod = ifelse(YearRemodAdd == YearBuilt, 0, 1),
RemodAdd = YearRemodAdd - YearBuilt) %>%
select(-YearRemodAdd)
grid.arrange(
ggplot(house_train, aes(YearBuilt,RemodAdd)) +
geom_point(aes(color = as.factor(isRemod))),
ggplot(house_test, aes(YearBuilt, RemodAdd)) +
geom_point(aes(color = as.factor(isRemod))),
ncol = 2
)
nzv <- nearZeroVar(house_train[, -length(house_train)],
freqCut = 99/1,
uniqueCut = 5,
saveMetrics= TRUE)
nzv[nzv$nzv,]
house_train[, rownames(nzv[nzv$nzv,])] <- NULL
house_test[, rownames(nzv[nzv$nzv,])] <- NULL
dim(house_train)
dim(house_test)
## use knn to impute all numerical varialbes
# preProc_numerical <- preProcess(house_train[, -length(house_train)],
#                                 method = c("knnImpute"))
# print(preProc_numerical)
# house_train[, -length(house_train)] <- predict(preProc_numerical,
#                               house_train[, -length(house_train)])
# house_test <- predict(preProc_numerical, house_test)
## Caret only handles numerical features
## use kNN to impute categorical features
house_train[, -length(house_train)] <- kNN(house_train[, -length(house_train)],
k=5)[,names(house_train)[-length(house_train)]]
house_test <- kNN(house_test, k=5)[,names(house_test)]
## Double check missingness with the following code
aggr(house_train, prop = F, numbers = T)
aggr(house_test, prop = F, numbers = T)
## Transform SalePrice to log scale
house_train$SalePrice <- log(house_train$SalePrice + 1)
save(house_train, house_test, file = "./house_preProc.RData")
## Perform single 80%/20% random split of house_train
library(caret)
set.seed(321)
trainIdx <- createDataPartition(house_train$SalePrice,
p = .8,
list = FALSE,
times = 1)
subTrain <- house_train[trainIdx,]
subTest <- house_train[-trainIdx,]
print(head(subTrain))
X1stFlrSF + X2ndFlrSF
head(subTrain)
head(subTrain)
rf.subTrain = randomForest(SalePrice ~ ., data = subTrain, subset = train, importance = TRUE)
library(randomForest)
rf.subTrain = randomForest(SalePrice ~ ., data = subTrain, subset = train, importance = TRUE)
rf.subTrain = randomForest(SalePrice ~ ., data = subTrain, subset = train, importance = TRUE)
rf.subTrain = randomForest(SalePrice ~ ., data = subTrain, subset = train, importance = TRUE)
rf.subTrain = randomForest(SalePrice ~ ., data = subTrain)
rf.subTrain = randomForest(SalePrice ~ ., data = subTrain, importance = TRUE)
rf.subTrain = randomForest(SalePrice ~ ., data = subTrain, importance = TRUE, subset=train)
rf.subTrain
importance(rf.subTrain)
varImpPlot(rf.boston)
varImpPlot(rf.subTrain)
head(subTrain)
mlr.subTrain = lm(SalePrice ~ ., data = subTrain)
summary(mlr.subTrain)
subTrain[MSSubClass,Neighborhood]
subTrain[(MSSubClass,Neighborhood)]
subTrain[,c(MSSubClass,Neighborhood)]
doPlots(subTrain, fun = plotDen, subTrain$YearBuilt , ncol = 2)
ggplot() + geom_freqpoly(mapping = subTrain$YearBuilt, data = subTrain, stat = "bin")
ggplot(subTrain, aes(SalePrice, colour = LandSlope)) +
geom_freqpoly(binwidth = 500)
ggplot(subTrain, aes(SalePrice) +
geom_freqpoly(binwidth = 500)
ggplot(subTrain, aes(SalePrice)) +
geom_freqpoly(binwidth = 500)
ggplot(subTrain, aes(SalePrice)) +    geom_freqpoly(binwidth = 500)
ggplot(subTrain, aes(exp(SalePrice))) +    geom_freqpoly(binwidth = 500)
ggplot(subTrain, aes(YearBuilt)) +    geom_freqpoly(binwidth = 500)
scatterplot(SalePrice ~ YearBuilt, data=subTrain,  xlab="Year Built", ylab="Sale Price", grid=FALSE)
library(car)
scatterplot(SalePrice ~ YearBuilt, data=subTrain,  xlab="Year Built", ylab="Sale Price", grid=FALSE)
scatterplot(SalePrice ~ LotFrontage, data=subTrain,  xlab="Year Built", ylab="Sale Price", grid=FALSE)
scatterplot(SalePrice ~ LotFrontage, data=subTrain,  xlab="LotFrontage", ylab="Sale Price", grid=FALSE)
influencePlot(mlr.subTrain)
vif(mlr.subTrain)
avPlots(mlr.subTrain)
en.models = glmnet(x, y, alpha = .5, lambda = grid)
plot(en.models, xvar = "lambda", label = TRUE, main = "EN Regression")
grid = 10^seq(5, -2, length = 100)
en.models = glmnet(x, y, alpha = .5, lambda = grid)
library(glmnet)
en.models = glmnet(x, y, alpha = .5, lambda = grid)
grid = 10^seq(5, -2, length = 100)
en.models = glmnet(x, y, alpha = .5, lambda = grid)
View(subTrain)
View(subTrain)
View(subTrain)
class(subTrain$MSSubClass)
class(subTrain$LotFrontage)
subTrain[-c(1,2)]
test.subTrain = subTrain[-c("Alley","LandSlope","LandContour","Heating",
"HeatingQC","CentralAir","Electrical","GarageCond","MiscFeature")]
subTrain[,-c("Alley","LandSlope","LandContour","Heating",
"HeatingQC","CentralAir","Electrical","GarageCond","MiscFeature")]
subTrain[,-c("Alley","LandSlope")]
subTrain[-c("Alley","LandSlope")]
within[subTrain,rm("Alley","LandSlope")]
within[subTrain,rm(Alley,LandSlope)]
drops = c("Alley","LandSlope","LandContour","Heating",
"HeatingQC","CentralAir","Electrical","GarageCond",
"MiscFeature")
subTrain[ , !(names(subTrain) %in% drops)]
grid = 10^seq(5, -2, length = 100)
x = model.matrix(SalePrice ~ ., subTrain)
y = subTrain$SalePrice
en.models = glmnet(x, y, alpha = .5, lambda = grid)
plot(en.models, xvar = "lambda", label = TRUE, main = "EN Regression")
set.seed(0)
cv.lasso.out = cv.glmnet(x, y,
lambda = grid, alpha = .5, nfolds = 10)
set.seed(0)
cv.lasso.out = cv.glmnet(x, y,
lambda = grid, alpha = .5, nfolds = 10)
plot(cv.lasso.out, main = "EN Regression\n")
bestlambda.lasso = cv.lasso.out$lambda.min
bestlambda.lasso
log(bestlambda.lasso)
summary(cv.lasso.out)
cv.lasso.out$glmnet.fit
cv.lasso.out$cvlo
cv.lasso.out$cvm
cv.lasso.out$cvup
cv.lasso.out$name
cv.lasso.out$glmmod
coef(glmmod)[, 10]
coef(cv.lasso.out)
subTrain %>% filter(.,subTrain$GrLivArea > 4000)
test.subTrain = subTrain %>% filter(.,subTrain$GrLivArea < 4000)
drops = c("Alley","LandSlope","LandContour","BsmtCond","Exterior1st","Exterior2nd",
"BldgType","HouseStyle","LotFrontage","Exterior2nd",
"Electrical","GarageCond","MiscFeature","Fence","YrSold","PavedDrive","Functional")
test.subTrain=test.subTrain[ , !(names(test.subTrain) %in% drops)]
dim(test.subTrain)
mlr.subTrain = lm(SalePrice ~ ., data = test.subTrain)
summary(mlr.subTrain)
rf.test.subTrain = randomForest(SalePrice ~ ., data = test.subTrain, importance = TRUE)
rf.test.subTrain
rf.subTrain
importance(rf.test.subTrain)
varImpPlot(rf.test.subTrain)
head(test.subTrain)
test.subTrain = subTrain %>% filter(.,subTrain$GrLivArea < 4000)
test.subTrain$Totalfinsf = GrLivArea + BsmtFinSF1 + BsmtFinSF2
test.subTrain$Totalfullbath = BsmtFullBath + FullBath
test.subTrain$Totalhalfbath = BsmtHalfBath + HalfBath
test.subTrain$Totalfinsf = test.subTrain$GrLivArea + test.subTrain$BsmtFinSF1 + test.subTrain$BsmtFinSF2
test.subTrain$Totalfullbath = test.subTrain$BsmtFullBath + test.subTrain$FullBath
test.subTrain$Totalhalfbath = test.subTrain$BsmtHalfBath + test.subTrain$HalfBath
drops = c("Alley","LandSlope","LandContour","BsmtCond","Exterior1st","Exterior2nd",
"BldgType","HouseStyle","LotFrontage","Exterior2nd",
"Electrical","GarageCond","MiscFeature","Fence","YrSold",
"PavedDrive","Functional","BsmtFullBath","BsmtHalfBath","FullBath","HalfBath",
"GrLivArea","X1stFlrSF","X2ndFlrSF")
test.subTrain=test.subTrain[ , !(names(test.subTrain) %in% drops)]
dim(test.subTrain)
set.seed(0)
rf.test.subTrain = randomForest(SalePrice ~ ., data = test.subTrain, importance = TRUE)
#subset=train
rf.subTrain
importance(rf.test.subTrain)
importance(rf.test.subTrain)
test.subTrain=test.subTrain[ , !(names(test.subTrain) %in% drops)]
dim(test.subTrain)
head(test.subTrain)
set.seed(0)
rf.test.subTrain = randomForest(SalePrice ~ ., data = test.subTrain, importance = TRUE)
rf.subTrain
importance(rf.test.subTrain)
